# Project 3 Summary

## Intro
Cardiac arrest is the medical term for when somebody's heart stops beating. This is a deadly condition, and public education campaigns to teach CPR to laypeople is aimed at increasing the rate and quality of bystander CPR in cases of out-of-hospital cardiac arrest, since immediate CPR is crucial in order to get cardiac arrest patients to the hospital alive. But how likely is survival after cardiac arrest? There is well-established medical literature around this question, and I wanted to build a binary classifier to predict survival after cardiac arrest, and to see if I could gain any insights and also to compare what I found with what has been published in the literature.

## Project design & data
After looking through different publicly available datasets, I ended up using the [Texas Hospital Discharge Data Public Use Data File](https://www.dshs.texas.gov/THCIC/Hospitals/Download.shtm) from 2011, which contains records for every hospital discharge across the state for that year. I loaded these records into Postgres, and then used Pandas to query the dataset for all the records that contained the diagnosis code for cardiac arrest. This yielded about 12 thousand records, from a pool of 3 million. About 30% of these patients were discharged alive, and the other 70% died.

So after identifying patient status as alive or dead at time of discharge as my target variable, I then looked at the columns of the dataset in order to identify useful predictors. I grouped these predictors into 4 broad categories. The first was general demographic information about each patient, including age, gender, race, and also the payment source for the hospitalization, e.g. Medicare, Medicaid, an HMO, PPO, self-pay, etc.

The second category was information about the hospital stay, for example the patient’s length of stay, what type of hospital they were admitted to, e.g. academic vs community, and also both the source of the admission (were they admitted from a nursing home, from another hospital, etc.) and the type of admission (emergent vs urgent vs elective).

The final two categories of predictors were the associated diagnosis codes and then the associated procedure codes. The data set includes all the diagnosis codes and procedure codes associated with each hospitalizaiton, and it also states whether or not a diagnosis was present prior to arrival, or if it occurred during the hospital stay. I collected the most common diagnoses and procedures codes in the data set, and used them as boolean predictors for the model. I ended up selecting the 20 most common present on arrival diagnoses and the 20 most common not-present on arrival diagnoses to come up with a total of 40, and removed the diagnosis code for cardiac arrest (since clearly everybody in my data set would have this, since it's how I created my subset in the first place!), and also removed the diagnosis code for 'do not resuscitate' status, which I did not realize was an ICD-9 diagnosis code, but apparently it is! Clearly DNR status would correlate very highly with patients who died, but this often is something that a patient's family or friends change once it become clear that a patient may not survive, so I did not think that it would be a meaningful predictor.

## Tools
I used Postgres to load the initial 3 million records from the Texas Hospital Discharge PUDF, and then queried my database for all the records containing the diagnosis code for cardiac arrest. This came to about 12 thousand records, which was easily manageable to work with in Jupyter notebooks. I also used the [Clinical Table Search Service](https://clinicaltables.nlm.nih.gov/apidoc/icd9cm_dx/v3/doc.html) from the US National Library of Medicine; they provide an API that is extremely easy, and I used it to to translate the ICD 9 diagnosis and procedure codes to the associated diagnosis or procedure so I could figure out which diagnoses/procedures to include in the model.

## Algorithms
In terms of modeling, I used sklearn’s gridsearch to evaluate several different algorithms, specifically logistic regression, decision tree, random forest, multilayer perceptron, and gradient-boosted trees. However, since many of my predictors were categorical, I had to one-hot encode most of my features in order to run sklearn’s methods. The performance of tree-based models can suffer when categorical data is one-hot encoded, and so I also used H2O to create and run models, since H2O is a machine learning platform that is able to handle categorical variables without one-hot encoding.

The metrics I used to evaluate these models was the cross-validation accuracy on the training set, and in order to pick the final model, I looked at the area under the ROC curve on the test set. For both sklearn and H2O, gradient boosted trees had the best performance; AUROC for the gradient-boosted tree for sklearn was 0.90098, and for H2O it was 0.90404.

Once I refit the h2o gradient boosted model on the full data set, the overall accuracy was 87%. The model’s recall for the ‘alive’ class was 74%, which means that it was able to correctly identify about ¾ of the people who survived to discharge.

In order to gain more insight, I looked at which features the tree and forest models considered important. Length of stay was the most important feature, which makes sense, since almost nobody who experiences a cardiac arrest has a short hospital stay unless they die. That means that the model could predict that people who had a short hospital stay most likely did not survive. The longer the length of stay, the more likely you are to survive to discharge.

Interestingly, cardiac procedures, specifically procedure to treat heart attacks, were next in terms of importance. There are many reasons why somebody’s heart might stop, but one of the most common is heart attacks. And so, what the model is telling us is that these procedures to treat heart attacks work, because patients who had them (presumably because they had a heart attack) were more likely to survive compared to the overall population of this data set.

I also looked at the coefficients of the logistic regression model, and one of the features it picked out was the diagnosis of anoxic brain injury. If you had anoxic brain injury before arrival, the odds ratio of not surviving to discharge was 4, and if you had anoxic brain injury in the hospital, the odds ratio was 3.

## Other work
I also looked briefly into doing PCA for dimensionality reduction, but after doing PCA and keeping the first 79 components (which captured 95% of the variance), the models did not perform significantly better on the PCA-transformed data, and in some cases performed worse. I also ran my data through TPOT, which is an open source [Python Machine Learning](https://epistasislab.github.io/tpot/) tool meant to optimize machine learning pipelines, but it took a long time to run the TPOTClassifier method in full on my method, and the abbreviated run came up with Xgboost's gradient-boosted tree with a set of parameters as the best model. However, when I took those parameters and put them into Xgboost and fit a model with them, the AUROC was not as good.

I also ran a few models for multilabel classification, specifically trying to predict between three classes: patients who are discharged home (which means that they are relatively healthy by the time of hospital discharge), patients who are discharged to rehabilitation, and patients who expire. I did not have time to run a gridsearch to tune different models for the multilabel classification problem, but when using the default parameters for all of the different models (logistic regression, decision tree, random forest, multilayer perceptron, and gradient-boosted trees), all of them had a significant drop in overall accuracy compared to the binary classification problems, with a rough accuracy of about 78 to 79%.

## Next steps
If I were going to continue to work on this model, I would want to try to figure out how to adapt the model in order to more effectively incorporate time/length of stay, so that it can provide predictions for patients on each day; as the model is currently built, it can 'cheat' by just looking at the length of stay and using that to make a prediction about whether or not a patient a patient survived to discharge, since as I noted before, most patients who have cardiac arrest do not have short hospital stays unless they die. What would be more useful would be to build a model that could provide a prediction for a specific patient on each day of their hospital stay, e.g. if they make it through day 1, what are the chances of surviving to discharge? If they make it to day 2, what are their chances of surviving to discharge, etc. Right now, the model requires a specific length of stay as a predictor, and this is clearly something that you don't know when the patient is currently admitted to the hospital.

Finally, I would be curious to know how this model that was fitted to data from 2011 would work on data from 2012, or 2013, or 2014? If recall for finding people who survive to discharge is getting worse, then does that mean that medical treatment is getting better? Conversely, if recall for finding people who die is getting worse, is medical treatment getting worse or are people getting sicker? The other thing that would be interesting would be to run the model on a data set from a different state, and to evaluate the recall for finding people who survive to discharge vs not in that population, and you could ask the same questions about whether or not it means medical care in that other state is comparable or worse to Texas, and/or if the population there is comparatively as 'healthy' or less healthy than Texas was in 2011.
