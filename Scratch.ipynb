{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load in the full X and y dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/full_X_y_df.pkl', 'rb') as picklefile:\n",
    "    [X, y] = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will separate predictors into 3 groups in order to keep track of them.\n",
    "\n",
    "target: patient status (at time of discharge)\n",
    "\n",
    "Predictor group 1: Personal demographics\n",
    "* Age\n",
    "* Gender\n",
    "* Race\n",
    "* Ethnicity\n",
    "* Primary payer (Medicare, Medicaid, private insurance, etc.)\n",
    "* Patient location (zip code/county vs public health region)\n",
    "\n",
    "Predictor group 2: Details about hospital stay\n",
    "* Day of the week patient admitted (difference between mortality in patients admitted on weekday vs weekend?)\n",
    "* Type of hospital patient is admitted to; academic vs private vs community vs critical access hospitals\n",
    "* Length of stay\n",
    "* Type of admission (urgent vs emergent vs elective)\n",
    "* Source of admission\n",
    "\n",
    "Predictor group 3: Medical/procedural\n",
    "* In-hospital vs out-of-hospital cardiac arrest (presumably, patients who had an admitting diagnosis of cardiac arrest experienced their arrest out-of-hospital, although there may be inaccuracy here if the patient was transferred from another hospital; need to account for this by looking at the source of admission)\n",
    "* Other associated diagnosis codes, e.g. diabetes, heart failure, etc.\n",
    "* Other associated procedural codes, e.g. heart surgery, mechanical ventilation, etc.\n",
    "* Can separate out the associated diagnosis codes to distinguish between the medical conditions that are present on arrival (e.g. chronic conditions) vs medical conditions that develop during the hospital stay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = 'pat_status'\n",
    "# index = 'record_id'\n",
    "\n",
    "personal_demographic_predictors = ['pat_age', 'sex_code', 'race', 'ethnicity', 'public_health_region', 'first_payment_src']\n",
    "hospital_stay_predictors = ['provider_name', 'type_of_admission', 'source_of_admission', 'admit_weekday', 'length_of_stay', 'type_of_bill']\n",
    "\n",
    "diag_codes_predictors = ['admitting_diagnosis']\n",
    "diag_codes_predictors.extend([col for col in X.columns if 'diag_code' in col])\n",
    "\n",
    "e_code_predictors = [col for col in X.columns if 'e_code' in col]\n",
    "\n",
    "proc_code_predictors = ['princ_surg_proc_code', 'princ_surg_proc_day', 'princ_icd9_code']\n",
    "proc_code_predictors.extend([col for col in X.columns if 'oth_surg' in col or 'oth_icd9' in col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform predictor columns\n",
    "\n",
    "We now have to transform the features of the dataframe into a form where we can actually run our different models on the data set. The majority of the predictor columns are categorical variables, and so we have to use pd.get_dummies (or sklearn onehotencoder) in order to get the dummy variables necessary for the categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pat_age</th>\n",
       "      <th>sex_code</th>\n",
       "      <th>race</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>public_health_region</th>\n",
       "      <th>first_payment_src</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>record_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120110921391</th>\n",
       "      <td>20</td>\n",
       "      <td>M</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>07</td>\n",
       "      <td>MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120110922505</th>\n",
       "      <td>11</td>\n",
       "      <td>F</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>07</td>\n",
       "      <td>MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120110921645</th>\n",
       "      <td>17</td>\n",
       "      <td>F</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>07</td>\n",
       "      <td>MA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120110919484</th>\n",
       "      <td>19</td>\n",
       "      <td>F</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>07</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120110919499</th>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>07</td>\n",
       "      <td>MA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pat_age sex_code race ethnicity public_health_region  \\\n",
       "record_id                                                           \n",
       "120110921391      20        M    4         2                   07   \n",
       "120110922505      11        F    3         2                   07   \n",
       "120110921645      17        F    4         2                   07   \n",
       "120110919484      19        F    4         2                   07   \n",
       "120110919499      16        M    5         1                   07   \n",
       "\n",
       "             first_payment_src  \n",
       "record_id                       \n",
       "120110921391                MA  \n",
       "120110922505                MA  \n",
       "120110921645                MA  \n",
       "120110919484                16  \n",
       "120110919499                MA  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[personal_demographic_predictors].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to fix the age ranges for patients who have a pat_age of 22 through 26, since they code patients who have HIV or drug/alcohol related diagnoses within broader age ranges. Those ranges are listed below:\n",
    "\n",
    "* 22: 0-17\n",
    "* 23: 18-44\n",
    "* 24: 45-64\n",
    "* 25: 65-74\n",
    "* 26: 75+\n",
    "\n",
    "Therefore, we can map these age ranges to the following:\n",
    "* 22 -> (encompasses 00 to 05) -> 05 (15-17)\n",
    "* 23 -> (encompasses 06 to 11) -> 09 (30-34)\n",
    "* 24 -> (encompasses 12 to 15) -> 14 (50-54)\n",
    "* 25 -> (encompasses 16 to 17) -> 17 (70-74)\n",
    "* 26 -> (encompasses 18 to 21) -> 18 (75-79)\n",
    "\n",
    "We'll also add another flag called 'hiv_drug' which is true for the patients with these values for their patient age, and false for all other patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to make X['pat_age'] numeric.\n",
    "\n",
    "X['pat_age'] = X['pat_age'].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a binary variable called 'hiv_drug' and initialize it to 0 for all patients.\n",
    "X['hiv_drug'] = 0\n",
    "\n",
    "# For every row where 'pat_age' is greater than 21, set the hiv_drug flag to drue.\n",
    "X.loc[X['pat_age'] > 21, 'hiv_drug'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now replace the patient ages for the hiv_drug patients with an approximate mapping to another age category.\n",
    "age_replace_dict = {22: 5, 23: 9, 24: 14, 25: 17, 26: 18}\n",
    "\n",
    "X = X.replace({'pat_age': age_replace_dict})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and create the dummified dataframe for the personal demographic predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pat_age',\n",
       " 'sex_code',\n",
       " 'race',\n",
       " 'ethnicity',\n",
       " 'public_health_region',\n",
       " 'first_payment_src']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "personal_demographic_predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = X[personal_demographic_predictors].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pat_age',\n",
       " 'sex_code',\n",
       " 'race',\n",
       " 'ethnicity',\n",
       " 'public_health_region',\n",
       " 'first_payment_src']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[col for col in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/24109779/running-get-dummies-on-several-dataframe-columns\n",
    "\n",
    "test = pd.concat([pd.get_dummies(test[col], prefix=col) for col in test if col != 'pat_age'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex_code_F</th>\n",
       "      <th>sex_code_M</th>\n",
       "      <th>sex_code_U</th>\n",
       "      <th>race_1</th>\n",
       "      <th>race_2</th>\n",
       "      <th>race_3</th>\n",
       "      <th>race_4</th>\n",
       "      <th>race_5</th>\n",
       "      <th>race_`</th>\n",
       "      <th>ethnicity_1</th>\n",
       "      <th>...</th>\n",
       "      <th>first_payment_src_CI</th>\n",
       "      <th>first_payment_src_HM</th>\n",
       "      <th>first_payment_src_LM</th>\n",
       "      <th>first_payment_src_MA</th>\n",
       "      <th>first_payment_src_MB</th>\n",
       "      <th>first_payment_src_MC</th>\n",
       "      <th>first_payment_src_OF</th>\n",
       "      <th>first_payment_src_VA</th>\n",
       "      <th>first_payment_src_WC</th>\n",
       "      <th>first_payment_src_ZZ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>record_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120110921391</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120110922505</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120110921645</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120110919484</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120110919499</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              sex_code_F  sex_code_M  sex_code_U  race_1  race_2  race_3  \\\n",
       "record_id                                                                  \n",
       "120110921391           0           1           0       0       0       0   \n",
       "120110922505           1           0           0       0       0       1   \n",
       "120110921645           1           0           0       0       0       0   \n",
       "120110919484           1           0           0       0       0       0   \n",
       "120110919499           0           1           0       0       0       0   \n",
       "\n",
       "              race_4  race_5  race_`  ethnicity_1          ...           \\\n",
       "record_id                                                  ...            \n",
       "120110921391       1       0       0            0          ...            \n",
       "120110922505       0       0       0            0          ...            \n",
       "120110921645       1       0       0            0          ...            \n",
       "120110919484       1       0       0            0          ...            \n",
       "120110919499       0       1       0            1          ...            \n",
       "\n",
       "              first_payment_src_CI  first_payment_src_HM  \\\n",
       "record_id                                                  \n",
       "120110921391                     0                     0   \n",
       "120110922505                     0                     0   \n",
       "120110921645                     0                     0   \n",
       "120110919484                     0                     0   \n",
       "120110919499                     0                     0   \n",
       "\n",
       "              first_payment_src_LM  first_payment_src_MA  \\\n",
       "record_id                                                  \n",
       "120110921391                     0                     1   \n",
       "120110922505                     0                     1   \n",
       "120110921645                     0                     1   \n",
       "120110919484                     0                     0   \n",
       "120110919499                     0                     1   \n",
       "\n",
       "              first_payment_src_MB  first_payment_src_MC  \\\n",
       "record_id                                                  \n",
       "120110921391                     0                     0   \n",
       "120110922505                     0                     0   \n",
       "120110921645                     0                     0   \n",
       "120110919484                     0                     0   \n",
       "120110919499                     0                     0   \n",
       "\n",
       "              first_payment_src_OF  first_payment_src_VA  \\\n",
       "record_id                                                  \n",
       "120110921391                     0                     0   \n",
       "120110922505                     0                     0   \n",
       "120110921645                     0                     0   \n",
       "120110919484                     0                     0   \n",
       "120110919499                     0                     0   \n",
       "\n",
       "              first_payment_src_WC  first_payment_src_ZZ  \n",
       "record_id                                                 \n",
       "120110921391                     0                     0  \n",
       "120110922505                     0                     0  \n",
       "120110921645                     0                     0  \n",
       "120110919484                     0                     0  \n",
       "120110919499                     0                     0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sex_code_F', 'sex_code_M', 'sex_code_U', 'race_1', 'race_2', 'race_3',\n",
       "       'race_4', 'race_5', 'race_`', 'ethnicity_1', 'ethnicity_2',\n",
       "       'ethnicity_`', 'public_health_region_01', 'public_health_region_02',\n",
       "       'public_health_region_03', 'public_health_region_04',\n",
       "       'public_health_region_05', 'public_health_region_06',\n",
       "       'public_health_region_07', 'public_health_region_08',\n",
       "       'public_health_region_09', 'public_health_region_10',\n",
       "       'public_health_region_11', 'first_payment_src_09',\n",
       "       'first_payment_src_11', 'first_payment_src_12', 'first_payment_src_13',\n",
       "       'first_payment_src_14', 'first_payment_src_15', 'first_payment_src_16',\n",
       "       'first_payment_src_AM', 'first_payment_src_BL', 'first_payment_src_CH',\n",
       "       'first_payment_src_CI', 'first_payment_src_HM', 'first_payment_src_LM',\n",
       "       'first_payment_src_MA', 'first_payment_src_MB', 'first_payment_src_MC',\n",
       "       'first_payment_src_OF', 'first_payment_src_VA', 'first_payment_src_WC',\n",
       "       'first_payment_src_ZZ'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_demographic_dict = {\n",
    "    'sex_code': pd.get_dummies(X['sex_code'], prefix='sex'),\n",
    "    'race': pd.get_dummies(X['race'], prefix='race'),\n",
    "    'ethnicity': pd.get_dummies(X['ethnicity'], prefix='ethn'),\n",
    "    'public_health_region': pd.get_dummies(X['public_health_region'], prefix='ph_reg'),\n",
    "    'first_payment_src': pd.get_dummies(X['first_payment_src'], prefix='first_pay')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_demographic_df = personal_demographic_dict['sex_code'].copy()\n",
    "personal_demographic_df = personal_demographic_df.join(personal_demographic_dict['race'])\\\n",
    ".join(personal_demographic_dict['ethnicity'])\\\n",
    ".join(personal_demographic_dict['public_health_region'])\\\n",
    ".join(personal_demographic_dict['first_payment_src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex_F</th>\n",
       "      <th>sex_M</th>\n",
       "      <th>sex_U</th>\n",
       "      <th>race_1</th>\n",
       "      <th>race_2</th>\n",
       "      <th>race_3</th>\n",
       "      <th>race_4</th>\n",
       "      <th>race_5</th>\n",
       "      <th>race_`</th>\n",
       "      <th>ethn_1</th>\n",
       "      <th>...</th>\n",
       "      <th>first_pay_CI</th>\n",
       "      <th>first_pay_HM</th>\n",
       "      <th>first_pay_LM</th>\n",
       "      <th>first_pay_MA</th>\n",
       "      <th>first_pay_MB</th>\n",
       "      <th>first_pay_MC</th>\n",
       "      <th>first_pay_OF</th>\n",
       "      <th>first_pay_VA</th>\n",
       "      <th>first_pay_WC</th>\n",
       "      <th>first_pay_ZZ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>record_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120110921391</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120110922505</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120110921645</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120110919484</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120110919499</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              sex_F  sex_M  sex_U  race_1  race_2  race_3  race_4  race_5  \\\n",
       "record_id                                                                   \n",
       "120110921391      0      1      0       0       0       0       1       0   \n",
       "120110922505      1      0      0       0       0       1       0       0   \n",
       "120110921645      1      0      0       0       0       0       1       0   \n",
       "120110919484      1      0      0       0       0       0       1       0   \n",
       "120110919499      0      1      0       0       0       0       0       1   \n",
       "\n",
       "              race_`  ethn_1      ...       first_pay_CI  first_pay_HM  \\\n",
       "record_id                         ...                                    \n",
       "120110921391       0       0      ...                  0             0   \n",
       "120110922505       0       0      ...                  0             0   \n",
       "120110921645       0       0      ...                  0             0   \n",
       "120110919484       0       0      ...                  0             0   \n",
       "120110919499       0       1      ...                  0             0   \n",
       "\n",
       "              first_pay_LM  first_pay_MA  first_pay_MB  first_pay_MC  \\\n",
       "record_id                                                              \n",
       "120110921391             0             1             0             0   \n",
       "120110922505             0             1             0             0   \n",
       "120110921645             0             1             0             0   \n",
       "120110919484             0             0             0             0   \n",
       "120110919499             0             1             0             0   \n",
       "\n",
       "              first_pay_OF  first_pay_VA  first_pay_WC  first_pay_ZZ  \n",
       "record_id                                                             \n",
       "120110921391             0             0             0             0  \n",
       "120110922505             0             0             0             0  \n",
       "120110921645             0             0             0             0  \n",
       "120110919484             0             0             0             0  \n",
       "120110919499             0             0             0             0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "personal_demographic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sex_F', 'sex_M', 'sex_U', 'race_1', 'race_2', 'race_3', 'race_4',\n",
       "       'race_5', 'race_`', 'ethn_1', 'ethn_2', 'ethn_`', 'ph_reg_01',\n",
       "       'ph_reg_02', 'ph_reg_03', 'ph_reg_04', 'ph_reg_05', 'ph_reg_06',\n",
       "       'ph_reg_07', 'ph_reg_08', 'ph_reg_09', 'ph_reg_10', 'ph_reg_11',\n",
       "       'first_pay_09', 'first_pay_11', 'first_pay_12', 'first_pay_13',\n",
       "       'first_pay_14', 'first_pay_15', 'first_pay_16', 'first_pay_AM',\n",
       "       'first_pay_BL', 'first_pay_CH', 'first_pay_CI', 'first_pay_HM',\n",
       "       'first_pay_LM', 'first_pay_MA', 'first_pay_MB', 'first_pay_MC',\n",
       "       'first_pay_OF', 'first_pay_VA', 'first_pay_WC', 'first_pay_ZZ'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "personal_demographic_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_demographic_df['pat_age'] = X['pat_age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_demographic_for_logr = personal_demographic_df.drop(['sex_U', 'race_`', 'ethn_`', 'ph_reg_11', 'first_pay_ZZ'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6878935768261965"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logr = LogisticRegression()\n",
    "\n",
    "logr.fit(personal_demographic_for_logr, y)\n",
    "\n",
    "logr.score(personal_demographic_for_logr, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6874212846347607"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_classifier = DummyClassifier(strategy='most_frequent')\n",
    "dummy_classifier.fit(personal_demographic_for_logr, y)\n",
    "dummy_classifier.score(personal_demographic_for_logr, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.796205919395466"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(personal_demographic_df, y)\n",
    "dtc.score(personal_demographic_df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7899086901763224"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(personal_demographic_df, y)\n",
    "rfc.score(personal_demographic_df, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using sklearn's labelencoder and onehotencoder to see if we can make a pipeline that automatically creates a sparse matrix for the demographic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = X[personal_demographic_predictors].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.replace({None: 'U'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "test = test.apply(le.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 22, 25, 32, 36, 48, 69])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_enc = OneHotEncoder(sparse=False)\n",
    "one_hot_enc.fit_transform(test)\n",
    "one_hot_enc.feature_indices_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "z = X['sex_code'].copy()\n",
    "z = z.replace({None: 'U'})\n",
    "\n",
    "z = le.fit_transform(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['F', 'M', 'U'], dtype=object)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_enc = OneHotEncoder()\n",
    "\n",
    "zed = one_hot_enc.fit_transform(np.asarray(z).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zed.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also make length of stay numeric as well.\n",
    "X['length_of_stay'] = X['length_of_stay'].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of each of the predictor groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[personal_demographic_predictors].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[hospital_stay_predictors].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[diag_codes_predictors].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[e_code_predictors].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[proc_code_predictors].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running logistic regression purely against age, it comes up with the same accuracy as the dummy classifier. It looks like this predictor never predicts that any patient is alive at the time of discharge; is this how the models should behave?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr = LogisticRegression()\n",
    "\n",
    "logr.fit(np.asarray(X['pat_age']).reshape(-1,1), y)\n",
    "\n",
    "y_pred = logr.predict(np.asarray(X['pat_age']).reshape(-1,1))\n",
    "\n",
    "print(set(y_pred))\n",
    "print(accuracy_score(y, y_pred))\n",
    "#logr.score(np.asarray(x_temp).reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = logr.predict_proba(np.asarray(X['pat_age']).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "dummy.fit(np.asarray(X['pat_age']).reshape(-1, 1), y)\n",
    "y_pred = dummy.predict(np.asarray(X['pat_age']).reshape(-1, 1))\n",
    "\n",
    "print(set(y_pred))\n",
    "print(accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = pd.DataFrame({'pat_age': X['pat_age'], 'pat_status': y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df.groupby('pat_status').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead = t_df[t_df.pat_status == 'expired'].copy()\n",
    "alive = t_df[t_df.pat_status == 'alive'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [dead['pat_age'], alive['pat_age']]\n",
    "labels = ['expired', 'alive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.boxplot(data, labels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy dataframe dictionary\n",
    "\n",
    "Most of the predictor variables are categorical variables, so we are going to have to dummify multiple variables. We will keep track of the each of the dummified categorical variables by using a dictionary to store the dataframe for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df_dictionary = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df_dictionary['sex_code'] = pd.get_dummies(X['sex_code'], prefix='sex')\n",
    "dummy_df_dictionary['race'] = pd.get_dummies(X['race'], prefix='race')\n",
    "dummy_df_dictionary['ethnicity'] = pd.get_dummies(X['ethnicity'], prefix='ethnicity')\n",
    "dummy_df_dictionary['first_payment_src'] = pd.get_dummies(X['first_payment_src'], prefix='first_pay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.fit(dummy_df_dictionary['first_payment_src'], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.score(dummy_df_dictionary['first_payment_src'], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal demographic predictors\n",
    "\n",
    "Now, let's look at the personal demographic predictors and figure out how this may need to get transformed.\n",
    "\n",
    "List of things that need to get fixed:\n",
    "1. There are 1125 records where the sex_code for these patients is missing. Need to figure out how to compensate for this, e.g. randomly assign half to male and the other half to female, vs. keeping 'none' as a third value for this field, vs. dropping all the rows without a gender. I don't think dropping all the rows without a gender is a good idea, since 1125 out of 12704 records do not have a gender, and I also don't think gender is a hugely influential factor where not having a gender means that that record can't be analyzed.\n",
    "2. Need to fix pat_age so that 22-26 gets recoded to the appropriate ages, and then you can also add an HIV/drug and alcohol use flag for those patients.\n",
    "3. Need to figure out how many dummy variables to use for first_payment_src; can I assign 3-4 dummy variables for the major types of insurance, and then ignore everything else? (everybody else who doesn't have any of those gets 0s for all those variables as a default level)\n",
    "4. Also need to figure out how to handle patient location data, as in which one would be the most useful; probably a combination of county and state? People who are out of state don't have a county location. Can again prob pick the top 10 most frequent counties as categorical variables; the alternative is to create ~200 dummy variables for each county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[personal_demographic_predictors].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X[personal_demographic_predictors][X[personal_demographic_predictors].isnull().any(axis=1)]\n",
    "\n",
    "no_sex_code = X[personal_demographic_predictors][X['sex_code'].isnull()]\n",
    "print(no_sex_code.shape)\n",
    "no_sex_code.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, so actually the reason why there's no gender for these patients is because they are either HIV or alcohol\n",
    "# use patients.\n",
    "no_sex_code.pat_age.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiv_alc_dr = X[personal_demographic_predictors][X.pat_age >= 22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alive_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = master_2011_df[master_2011_df['record_id'] == '120110918756']\n",
    "print(z[personal_demographic_predictors])\n",
    "z['pat_status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnosis Codes\n",
    "\n",
    "Let's deal with the diagnosis codes first. So what we can do is form tuples out of the diagnosis codes and whether or not they are present on arrival (poa is yes vs any other value, e.g. no or none), then create a global dictionary of these tuples and keep a count of how many times those tuples have occurred. Then you can pick the top 10 or 20 tuples that occur most frequently, and then use them as dummy variables (if they are present or absent).\n",
    "\n",
    "You could also look at 4 different categories of tuple/poa pairs. 1) most common diag code + present on arrival, and pick the chronic conditions, 2) most common diag codes + *not* present on arrival, looking at the things that happen while the patient is in the hospital, and then 3) & 4) pick the top 10 of each for patients who did survive vs patients who did not survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's make tuple pairs for each diagnosis and whether or not it was present on arrival, for each\n",
    "# diagnosis column. First we get the names of the diagnosis columns, and then the names of the poa columns.\n",
    "\n",
    "diag_codes_cols = (X.filter(regex=r'(^princ_diag)|(^oth_diag)', axis=1)).columns\n",
    "diag_codes_poa_cols = (X.filter(regex=r'(^poa_princ)|(^poa_oth)', axis=1)).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll pair the diag columns with the poa columns and store them as tuples, and then also join the\n",
    "# paired column names together to come up with the name for the new tuple column.\n",
    "paired_diag_poa = list(zip(diag_codes_cols, diag_codes_poa_cols))\n",
    "paired_diag_poa_names = [pair[0] + '_AND_' + pair[1] for pair in paired_diag_poa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll create a mini-dataframe with the first two columns of X, just so that we can have a frame to add our\n",
    "# tuple diag/POA columns to.\n",
    "diag_poa_frame = X[['discharge', 'thcic_id']].copy()\n",
    "\n",
    "# Iterate through the paired diagnosis/POA column names with an index, and zip those two columns together into\n",
    "# a new column with that name.\n",
    "for i, paired_diag_poa_col in enumerate(paired_diag_poa_names):\n",
    "    diag_poa_frame[paired_diag_poa_col] = list(X[list(paired_diag_poa[i])].itertuples(index=False, name=None))\n",
    "\n",
    "# Now that we've created the diagnosis/POA tuple columns, we can drop the first two columns copied from X.\n",
    "diag_poa_frame.drop(['discharge', 'thcic_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of all the diagnosis tuples by making a list of all rows\n",
    "diag_poa_lists = diag_poa_frame.apply(list, axis=1)\n",
    "\n",
    "# We now add this list of all the diagnosis/POA tuples as a new column to X.\n",
    "X['diag_poa_list'] = diag_poa_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can make a list of lists from the diag/poa tuples for each record.\n",
    "diag_poas_list_of_lists = X['diag_poa_list'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's flatten out the list of lists, and also sort the diagnosis/poa tuples into lists based on whether or not\n",
    "# they were present on arrival or not.\n",
    "\n",
    "not_poa_diags = []\n",
    "poa_diags = []\n",
    "\n",
    "for sublist in diag_poas_list_of_lists:\n",
    "    for item in sublist:\n",
    "        if item[0]:\n",
    "            if item[1] == 'Y':\n",
    "                poa_diags.append(item)\n",
    "            else:\n",
    "                not_poa_diags.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "poa_diags_counter = Counter(poa_diags)\n",
    "not_poa_diags_counter = Counter(not_poa_diags)\n",
    "\n",
    "N = 30\n",
    "\n",
    "# We can look at the top N most common present on arrival diagnosis codes:\n",
    "poa_diags_most_common_dict = dict(poa_diags_counter.most_common(N))\n",
    "\n",
    "# And we can look at the top N most common *not* present on arrival diagnosis codes:\n",
    "not_poa_diags_most_common_dict = dict(not_poa_diags_counter.most_common(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('4275', 'N'): 6075,\n",
       " ('51881', 'N'): 2349,\n",
       " ('V4986', 'N'): 1340,\n",
       " ('V667', None): 1224,\n",
       " ('5849', 'N'): 1134,\n",
       " ('V1582', None): 1122,\n",
       " ('2762', 'N'): 886,\n",
       " ('V4581', None): 857,\n",
       " ('0389', 'N'): 820,\n",
       " ('99592', 'N'): 788,\n",
       " ('3481', 'N'): 782,\n",
       " ('412', None): 780,\n",
       " ('2768', 'N'): 777,\n",
       " ('V5869', None): 731,\n",
       " ('4271', 'N'): 723,\n",
       " ('V4511', None): 714,\n",
       " ('V5866', None): 704,\n",
       " ('V1254', None): 694,\n",
       " ('5070', 'N'): 682,\n",
       " ('V4582', None): 664,\n",
       " ('4275', None): 651,\n",
       " ('2760', 'N'): 635,\n",
       " ('78552', 'N'): 627,\n",
       " ('V5867', None): 609,\n",
       " ('42741', 'N'): 600,\n",
       " ('42789', 'N'): 596,\n",
       " ('486', 'N'): 554,\n",
       " ('5180', 'N'): 547,\n",
       " ('78551', 'N'): 539,\n",
       " ('2851', 'N'): 521}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_poa_diags_most_common_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_tables_query = \"https://clinicaltables.nlm.nih.gov/api/icd9cm_dx/v3/search?terms={}\"\n",
    "\n",
    "# poa_diag_most_common_text_dict = {}\n",
    "\n",
    "# for key, value in poa_diags_most_common_dict.items():\n",
    "#     response = requests.get(clinical_tables_query.format(key[0]))\n",
    "#     val = response.json()\n",
    "#     dx = val[3][0][1].strip()\n",
    "#     poa_diag_most_common_text_dict[dx] = value\n",
    "\n",
    "not_poa_diag_most_common_text_dict = {}\n",
    "\n",
    "for key, value in not_poa_diags_most_common_dict.items():\n",
    "    response = requests.get(clinical_tables_query.format(key[0]))\n",
    "    val = response.json()\n",
    "    dx = val[3][0][1].strip()\n",
    "    if dx in not_poa_diag_most_common_text_dict:\n",
    "        not_poa_diag_most_common_text_dict[dx] += value\n",
    "    else:\n",
    "        not_poa_diag_most_common_text_dict[dx] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cardiac arrest': 5052,\n",
       " 'Acute respiratory failure': 4747,\n",
       " 'Unspecified essential hypertension': 3722,\n",
       " 'Congestive heart failure, unspecified': 3525,\n",
       " 'Coronary atherosclerosis of native coronary artery': 3087,\n",
       " 'Other and unspecified hyperlipidemia': 2801,\n",
       " 'Diabetes mellitus without mention of complication, type II or unspecified type, not stated as uncontrolled': 2661,\n",
       " 'Acute kidney failure, unspecified': 2619,\n",
       " 'Anoxic brain damage': 2540,\n",
       " 'Acidosis': 2269,\n",
       " 'Atrial fibrillation': 2115,\n",
       " 'Unspecified septicemia': 1738,\n",
       " 'Anemia, unspecified': 1652,\n",
       " 'Severe sepsis': 1588,\n",
       " 'Hypertensive chronic kidney disease, unspecified, with chronic kidney disease stage I through stage IV, or unspecified': 1507,\n",
       " 'End stage renal disease': 1501,\n",
       " 'Pneumonia, organism unspecified': 1453,\n",
       " 'Chronic airway obstruction, not elsewhere classified': 1403,\n",
       " 'Tobacco use disorder': 1401,\n",
       " 'Unspecified acquired hypothyroidism': 1331,\n",
       " 'Septic shock': 1273,\n",
       " 'Hypertensive chronic kidney disease, unspecified, with chronic kidney disease stage V or end stage renal disease': 1246,\n",
       " 'Urinary tract infection, site not specified': 1236,\n",
       " 'Hyperpotassemia': 1135,\n",
       " 'Chronic kidney disease, unspecified': 1097,\n",
       " 'Subendocardial infarction, initial episode of care': 1093,\n",
       " 'Ventricular fibrillation': 1086,\n",
       " 'Pneumonitis due to inhalation of food or vomitus': 1082,\n",
       " 'Coronary atherosclerosis of unspecified type of vessel, native or graft': 1070,\n",
       " 'Esophageal reflux': 1034}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Cardiac arrest': 6726,\n",
       " 'Acute respiratory failure': 2349,\n",
       " 'Do not resuscitate status': 1340,\n",
       " 'Encounter for palliative care': 1224,\n",
       " 'Acute kidney failure, unspecified': 1134,\n",
       " 'Personal history of tobacco use': 1122,\n",
       " 'Acidosis': 886,\n",
       " 'Aortocoronary bypass status': 857,\n",
       " 'Unspecified septicemia': 820,\n",
       " 'Severe sepsis': 788,\n",
       " 'Anoxic brain damage': 782,\n",
       " 'Old myocardial infarction': 780,\n",
       " 'Hypopotassemia': 777,\n",
       " 'Long-term (current) use of other medications': 731,\n",
       " 'Paroxysmal ventricular tachycardia': 723,\n",
       " 'Renal dialysis status': 714,\n",
       " 'Long-term (current) use of aspirin': 704,\n",
       " 'Personal history of transient ischemic attack (TIA), and cerebral infarction without residual deficits': 694,\n",
       " 'Pneumonitis due to inhalation of food or vomitus': 682,\n",
       " 'Percutaneous transluminal coronary angioplasty status': 664,\n",
       " 'Hyperosmolality and/or hypernatremia': 635,\n",
       " 'Septic shock': 627,\n",
       " 'Long-term (current) use of insulin': 609,\n",
       " 'Ventricular fibrillation': 600,\n",
       " 'Other specified cardiac dysrhythmias': 596,\n",
       " 'Pneumonia, organism unspecified': 554,\n",
       " 'Pulmonary collapse': 547,\n",
       " 'Cardiogenic shock': 539,\n",
       " 'Acute posthemorrhagic anemia': 521}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(poa_diag_most_common_text_dict)\n",
    "display(not_poa_diag_most_common_text_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure codes\n",
    "\n",
    "We should also track the procedure codes that are the most common for patients. This is a similar process as for the diagnosis codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_codes_cols = (X.filter(regex=r'(icd9_code)', axis=1)).columns\n",
    "proc_day_cols = (X.filter(regex=r'(proc_day)', axis=1)).columns\n",
    "\n",
    "paired_proc_day = list(zip(proc_codes_cols, proc_day_cols))\n",
    "paired_proc_day_names = [pair[0] + '_AND_day' for pair in paired_proc_day]\n",
    "\n",
    "X[proc_day_cols] = X[proc_day_cols].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_day_frame = X[['discharge', 'thcic_id']].copy()\n",
    "\n",
    "for i, paired_proc_day_col in enumerate(paired_proc_day_names):\n",
    "    proc_day_frame[paired_proc_day_col] = list(X[list(paired_proc_day[i])].itertuples(index=False, name=None))\n",
    "\n",
    "proc_day_frame.drop(['discharge', 'thcic_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of all the procedure tuples by making a list of all rows\n",
    "proc_day_lists = proc_day_frame.apply(list, axis=1)\n",
    "\n",
    "# We now add this list of all the diagnosis/POA tuples as a new column to X.\n",
    "X['proc_day_list'] = proc_day_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get most common procedures\n",
    "\n",
    "Steps to get the most common procedures:\n",
    "1. Make a list of lists of all the procedures\n",
    "2. Flatten list\n",
    "3. Create counter object that gets the most common N procedures\n",
    "4. Look up the value of these procedure codes in ICD 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Insertion of endotracheal tube': 7356,\n",
       " 'Continuous invasive mechanical ventilation for less than 96 consecutive hours': 6208,\n",
       " 'Cardiopulmonary resuscitation, not otherwise specified': 4933,\n",
       " 'Venous catheterization, not elsewhere classified': 4672,\n",
       " 'Transfusion of packed cells': 3107,\n",
       " 'Continuous invasive mechanical ventilation for 96 consecutive hours or more': 2996,\n",
       " 'Arterial catheterization': 1945,\n",
       " 'Hemodialysis': 1763,\n",
       " 'Coronary arteriography using two catheters': 1746,\n",
       " 'Left heart cardiac catheterization': 1599,\n",
       " 'Angiocardiography of left heart structures': 1313,\n",
       " 'Infusion of vasopressor agent': 1068,\n",
       " 'Transfusion of other serum': 1041,\n",
       " 'Central venous catheter placement with guidance': 1013,\n",
       " 'Percutaneous transluminal coronary angioplasty [PTCA]': 967,\n",
       " 'Venous catheterization for renal dialysis': 911,\n",
       " 'Procedure on single vessel': 792,\n",
       " 'Other electric countershock of heart': 756,\n",
       " 'Temporary tracheostomy': 653,\n",
       " 'Transfusion of platelets': 643,\n",
       " 'Implant of pulsation balloon': 600,\n",
       " 'Non-invasive mechanical ventilation': 594,\n",
       " 'Diagnostic ultrasound of heart': 573,\n",
       " 'Insertion of drug-eluting coronary artery stent(s)': 529,\n",
       " 'Insertion of one vascular stent': 518,\n",
       " 'Closed [endoscopic] biopsy of bronchus': 512,\n",
       " 'Percutaneous [endoscopic] gastrostomy [PEG]': 508,\n",
       " 'Insertion of intercostal catheter for drainage': 450,\n",
       " 'Extracorporeal circulation auxiliary to open heart surgery': 420,\n",
       " 'Parenteral infusion of concentrated nutritional substances': 363}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's also make a list of all the procedures associated with a particular discharge.\n",
    "proc_lists = X[proc_codes_cols].apply(list, axis=1)\n",
    "\n",
    "# Let's add the list of procedures back to the original dataframe.\n",
    "proc_lists = proc_lists.apply(set).apply(list)\n",
    "\n",
    "#proc_lists = [list(filter(None, x)) for x in z]\n",
    "\n",
    "X['proc_list'] = proc_lists\n",
    "\n",
    "all_proc_list = []\n",
    "\n",
    "for sublist in proc_lists:\n",
    "    for item in sublist:\n",
    "        if item:\n",
    "            all_proc_list.append(item)\n",
    "\n",
    "all_proc_count = Counter(all_proc_list)\n",
    "\n",
    "all_proc_dict = dict(all_proc_count.most_common(N))\n",
    "\n",
    "clinical_tables_proc_query = \"https://clinicaltables.nlm.nih.gov/api/icd9cm_sg/v3/search?terms={}\"\n",
    "\n",
    "all_proc_text_dict = {}\n",
    "\n",
    "for key, value in all_proc_dict.items():\n",
    "    response = requests.get(clinical_tables_proc_query.format(key))\n",
    "    val = response.json()\n",
    "    dx = val[3][0][1].strip()\n",
    "    all_proc_text_dict[dx] = value\n",
    "\n",
    "all_proc_text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can make a list of lists from the proc/day tuples for each record.\n",
    "proc_day_list_of_lists = X['proc_day_list'].tolist()\n",
    "\n",
    "# Let's flatten out the list of lists, and then pick the most common N proc/day tuples that are present.\n",
    "\n",
    "proc_day_pairs = []\n",
    "\n",
    "for sublist in proc_day_list_of_lists:\n",
    "    for item in sublist:\n",
    "        if item[0]:\n",
    "            proc_day_pairs.append(item)\n",
    "\n",
    "proc_day_count = Counter(proc_day_pairs)\n",
    "\n",
    "proc_day_dict = dict(proc_day_count.most_common(30))\n",
    "\n",
    "proc_day_dict\n",
    "\n",
    "clinical_tables_proc_query = \"https://clinicaltables.nlm.nih.gov/api/icd9cm_sg/v3/search?terms={}\"\n",
    "\n",
    "proc_day_text_dict = {}\n",
    "\n",
    "for key, value in proc_day_dict.items():\n",
    "    response = requests.get(clinical_tables_proc_query.format(key[0]))\n",
    "    val = response.json()\n",
    "    dx = val[3][0][1].strip()\n",
    "    proc_day_text_dict[dx] = value\n",
    "\n",
    "proc_day_text_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictors that are important are likely going to be different for in-hospital vs out-of-hospital cardiac arrests; how to account for this? could split the populations manually.\n",
    "\n",
    "The columns that I would be interested in as predictors:\n",
    "* provider_name (also the same as thcic_id)\n",
    "* type_of_admission (exclude 4 which is Newborn, and 9 which is unknown? prob okay to include 9)\n",
    "* source_of_admission\n",
    "* patient_state (can likely separate into in-state vs out of state patients)\n",
    "* pat_zip (patient zip code)\n",
    "* county (fips code)\n",
    "* public_health_region\n",
    "* sex_code\n",
    "* race\n",
    "* ethnicity\n",
    "* admit_weekday (weekend vs weekday)\n",
    "* length_of_stay\n",
    "* pat_age (patient age on day of discharge)\n",
    "* first_payment_src\n",
    "* secondary_payment_src\n",
    "* admitting_diagnosis (this would be how you would split the data between out-of-hospital vs in-hospital cardiac arrests)\n",
    "* all of the other diagnoses codes (this is how you can figure out what are the comorbidities for these patients; you could use the most common ?chronic comorbidities as predictors, e.g. the presence or absence of them in any of the diagnoses codes; will have to do rearranging of this dataframe in order to come up with these 'flag' variables)\n",
    "* e-code (again, you should probably look to see if there are external injury codes that come up particularly frequently, and then use those as flag variables)\n",
    "* procedure codes (again, look at the procedures that are most frequently performed on these patients, use them as flags; these primarily serve as a surrogate marker for how severe the disease is, e.g. if a patient requires a cardiac cath then they're sick, if they require a CABG then they're probably sicker, etc.) --> if you have time to get really granular about it, there are fields that records on which day a procedure occurred, you could try to tie that in somehow.\n",
    "* MS-MDC (major diagnosis code; not sure if it will add more info than already provided in the diagnosis codes, but maybe?)\n",
    "* MS-DRG (diagnosis-related group)\n",
    "* risk_mortality (Assignment of a risk of mortality score from the All Patient Refined (APR) Diagnosis Related\n",
    "* Group (DRG) from the 3M APR-DRG Grouper. Indicates the likelihood of dying.)\n",
    "* illness_severity (Assignment of a severity of illness score from the All Patient Refined (APR) Diagnosis Related\n",
    "* Group (DRG) from the 3M APR-DRG Grouper. Indicates the extent of physiologic decompensation.)\n",
    "* attending_physician\n",
    "* operating_physician\n",
    "\n",
    "Target variable:\n",
    "* pat_status -> for a binary classifier, you could separate it out into either expired (20, 40, 41, 42) or hospice (50, 51) vs alive at discharge. You could probably more usefully separate it into 3 classes, which is expired, discharged home, and discharged to some other facility (but probably more specifically a SNF or inpatient rehability facility). You should probably pull all the rows where the patients experienced a cardiac arrest, and see what their patient status is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df['oth_diag_code_1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patient = X[diag_codes_predictors].loc['120110924439']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patient_dxes = test_patient.filter(regex=r'(^adm)|(^princ)|(^oth)', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_tables_query = \"https://clinicaltables.nlm.nih.gov/api/icd9cm_dx/v3/search?terms={}\"\n",
    "\n",
    "for dx in test_patient_dxes:\n",
    "    if dx:\n",
    "        response = requests.get(clinical_tables_query.format(dx))\n",
    "        val = response.json()\n",
    "        print(val[3][0][1].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
